{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv5 ðŸš€ by Ultralytics, AGPL-3.0 license"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "Export a YOLOv5 PyTorch model to other formats. TensorFlow exports authored by https://github.com/zldrobit<br>\n",
    "Format                      | `export.py --include`         | Model<br>\n",
    "---                         | ---                           | ---<br>\n",
    "PyTorch                     | -                             | yolov5s.pt<br>\n",
    "TorchScript                 | `torchscript`                 | yolov5s.torchscript<br>\n",
    "ONNX                        | `onnx`                        | yolov5s.onnx<br>\n",
    "OpenVINO                    | `openvino`                    | yolov5s_openvino_model/<br>\n",
    "TensorRT                    | `engine`                      | yolov5s.engine<br>\n",
    "CoreML                      | `coreml`                      | yolov5s.mlmodel<br>\n",
    "TensorFlow SavedModel       | `saved_model`                 | yolov5s_saved_model/<br>\n",
    "TensorFlow GraphDef         | `pb`                          | yolov5s.pb<br>\n",
    "TensorFlow Lite             | `tflite`                      | yolov5s.tflite<br>\n",
    "TensorFlow Edge TPU         | `edgetpu`                     | yolov5s_edgetpu.tflite<br>\n",
    "TensorFlow.js               | `tfjs`                        | yolov5s_web_model/<br>\n",
    "PaddlePaddle                | `paddle`                      | yolov5s_paddle_model/<br>\n",
    "Requirements:<br>\n",
    "    $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime openvino-dev tensorflow-cpu  # CPU<br>\n",
    "    $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime-gpu openvino-dev tensorflow  # GPU<br>\n",
    "Usage:<br>\n",
    "    $ python export.py --weights yolov5s.pt --include torchscript onnx openvino engine coreml tflite ...<br>\n",
    "Inference:<br>\n",
    "    $ python detect.py --weights yolov5s.pt                 # PyTorch<br>\n",
    "                                 yolov5s.torchscript        # TorchScript<br>\n",
    "                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn<br>\n",
    "                                 yolov5s_openvino_model     # OpenVINO<br>\n",
    "                                 yolov5s.engine             # TensorRT<br>\n",
    "                                 yolov5s.mlmodel            # CoreML (macOS-only)<br>\n",
    "                                 yolov5s_saved_model        # TensorFlow SavedModel<br>\n",
    "                                 yolov5s.pb                 # TensorFlow GraphDef<br>\n",
    "                                 yolov5s.tflite             # TensorFlow Lite<br>\n",
    "                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU<br>\n",
    "                                 yolov5s_paddle_model       # PaddlePaddle<br>\n",
    "TensorFlow.js:<br>\n",
    "    $ cd .. && git clone https://github.com/zldrobit/tfjs-yolov5-example.git && cd tfjs-yolov5-example<br>\n",
    "    $ npm install<br>\n",
    "    $ ln -s ../../yolov5/yolov5s_web_model public/yolov5s_web_model<br>\n",
    "    $ npm start<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import contextlib\n",
    "import json\n",
    "import os\n",
    "import platform\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FILE = Path(__file__).resolve()\n",
    "# ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
    "# if str(ROOT) not in sys.path:\n",
    "#     sys.path.append(str(ROOT))  # add ROOT to PATH\n",
    "# if platform.system() != \"Windows\":\n",
    "#     ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: .\n"
     ]
    }
   ],
   "source": [
    "# Obtener la ruta del directorio actual del cuaderno\n",
    "NOTEBOOK_DIR = Path(os.path.abspath('')).resolve()\n",
    "\n",
    "# Asegurarse de que la ruta del cuaderno estÃ© en el PATH\n",
    "if str(NOTEBOOK_DIR) not in sys.path:\n",
    "    sys.path.append(str(NOTEBOOK_DIR))\n",
    "\n",
    "# Establecer la variable ROOT como el directorio padre del cuaderno\n",
    "ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "# Agregar la carpeta actual del cuaderno a ROOT de forma dinÃ¡mica\n",
    "current_folder_name = os.path.basename(NOTEBOOK_DIR)\n",
    "ROOT = ROOT / current_folder_name\n",
    "\n",
    "# Asegurarse de que ROOT estÃ© en el PATH\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "# Convertir ROOT a una ruta relativa desde el directorio de trabajo actual\n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))\n",
    "\n",
    "# Imprimir el resultado para verificar\n",
    "print(\"ROOT:\", ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.1.47-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics) (3.8.3)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics) (4.9.0.80)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics) (10.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics) (1.12.0)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics) (2.1.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics) (0.16.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics) (4.66.2)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics) (5.9.8)\n",
      "Collecting py-cpuinfo (from ultralytics)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting thop>=0.1.1 (from ultralytics)\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics) (1.5.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Downloading ultralytics-8.1.47-py3-none-any.whl (750 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m750.4/750.4 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: py-cpuinfo, thop, ultralytics\n",
      "Successfully installed py-cpuinfo-9.0.0 thop-0.1.1.post2209072238 ultralytics-8.1.47\n",
      "/home/ec2-user/SageMaker/elecnor/notebooks/ImagenesEPI/yolov5/utils/general.py\n"
     ]
    }
   ],
   "source": [
    "from models.experimental import attempt_load\n",
    "from models.yolo import ClassificationModel, Detect, DetectionModel, SegmentationModel\n",
    "from utils.dataloaders import LoadImages\n",
    "from utils.general import (\n",
    "    LOGGER,\n",
    "    Profile,\n",
    "    check_dataset,\n",
    "    check_img_size,\n",
    "    check_requirements,\n",
    "    check_version,\n",
    "    check_yaml,\n",
    "    colorstr,\n",
    "    file_size,\n",
    "    get_default_args,\n",
    "    print_args,\n",
    "    url2file,\n",
    "    yaml_save,\n",
    ")\n",
    "from utils.torch_utils import select_device, smart_inference_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MACOS = platform.system() == \"Darwin\"  # macOS environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class iOSModel(torch.nn.Module):\n",
    "    def __init__(self, model, im):\n",
    "        \"\"\"Initializes an iOS compatible model with normalization based on image dimensions.\"\"\"\n",
    "        super().__init__()\n",
    "        b, c, h, w = im.shape  # batch, channel, height, width\n",
    "        self.model = model\n",
    "        self.nc = model.nc  # number of classes\n",
    "        if w == h:\n",
    "            self.normalize = 1.0 / w\n",
    "        else:\n",
    "            self.normalize = torch.tensor([1.0 / w, 1.0 / h, 1.0 / w, 1.0 / h])  # broadcast (slower, smaller)\n",
    "            # np = model(im)[0].shape[1]  # number of points\n",
    "            # self.normalize = torch.tensor([1. / w, 1. / h, 1. / w, 1. / h]).expand(np, 4)  # explicit (faster, larger)\n",
    "    def forward(self, x):\n",
    "        \"\"\"Runs forward pass on the input tensor, returning class confidences and normalized coordinates.\"\"\"\n",
    "        xywh, conf, cls = self.model(x)[0].squeeze().split((4, 1, self.nc), 1)\n",
    "        return cls * conf, xywh * self.normalize  # confidence (3780, 80), coordinates (3780, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def export_formats():\n",
    "    \"\"\"Returns a DataFrame of supported YOLOv5 model export formats and their properties.\"\"\"\n",
    "    x = [\n",
    "        [\"PyTorch\", \"-\", \".pt\", True, True],\n",
    "        [\"TorchScript\", \"torchscript\", \".torchscript\", True, True],\n",
    "        [\"ONNX\", \"onnx\", \".onnx\", True, True],\n",
    "        [\"OpenVINO\", \"openvino\", \"_openvino_model\", True, False],\n",
    "        [\"TensorRT\", \"engine\", \".engine\", False, True],\n",
    "        [\"CoreML\", \"coreml\", \".mlmodel\", True, False],\n",
    "        [\"TensorFlow SavedModel\", \"saved_model\", \"_saved_model\", True, True],\n",
    "        [\"TensorFlow GraphDef\", \"pb\", \".pb\", True, True],\n",
    "        [\"TensorFlow Lite\", \"tflite\", \".tflite\", True, False],\n",
    "        [\"TensorFlow Edge TPU\", \"edgetpu\", \"_edgetpu.tflite\", False, False],\n",
    "        [\"TensorFlow.js\", \"tfjs\", \"_web_model\", False, False],\n",
    "        [\"PaddlePaddle\", \"paddle\", \"_paddle_model\", True, True],\n",
    "    ]\n",
    "    return pd.DataFrame(x, columns=[\"Format\", \"Argument\", \"Suffix\", \"CPU\", \"GPU\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def try_export(inner_func):\n",
    "    \"\"\"Decorator @try_export for YOLOv5 model export functions that logs success/failure, time taken, and file size.\"\"\"\n",
    "    inner_args = get_default_args(inner_func)\n",
    "    def outer_func(*args, **kwargs):\n",
    "        prefix = inner_args[\"prefix\"]\n",
    "        try:\n",
    "            with Profile() as dt:\n",
    "                f, model = inner_func(*args, **kwargs)\n",
    "            LOGGER.info(f\"{prefix} export success âœ… {dt.t:.1f}s, saved as {f} ({file_size(f):.1f} MB)\")\n",
    "            return f, model\n",
    "        except Exception as e:\n",
    "            LOGGER.info(f\"{prefix} export failure âŒ {dt.t:.1f}s: {e}\")\n",
    "            return None, None\n",
    "    return outer_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@try_export\n",
    "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\")):\n",
    "    \"\"\"Exports YOLOv5 model to TorchScript format, optionally optimized for mobile, with image shape and stride\n",
    "    metadata.\n",
    "    \"\"\"\n",
    "    LOGGER.info(f\"\\n{prefix} starting export with torch {torch.__version__}...\")\n",
    "    f = file.with_suffix(\".torchscript\")\n",
    "    ts = torch.jit.trace(model, im, strict=False)\n",
    "    d = {\"shape\": im.shape, \"stride\": int(max(model.stride)), \"names\": model.names}\n",
    "    extra_files = {\"config.txt\": json.dumps(d)}  # torch._C.ExtraFilesMap()\n",
    "    if optimize:  # https://pytorch.org/tutorials/recipes/mobile_interpreter.html\n",
    "        optimize_for_mobile(ts)._save_for_lite_interpreter(str(f), _extra_files=extra_files)\n",
    "    else:\n",
    "        ts.save(str(f), _extra_files=extra_files)\n",
    "    return f, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@try_export\n",
    "def export_onnx(model, im, file, opset, dynamic, simplify, prefix=colorstr(\"ONNX:\")):\n",
    "    \"\"\"Exports a YOLOv5 model to ONNX format with dynamic axes and optional simplification.\"\"\"\n",
    "    check_requirements(\"onnx>=1.12.0\")\n",
    "    import onnx\n",
    "    LOGGER.info(f\"\\n{prefix} starting export with onnx {onnx.__version__}...\")\n",
    "    f = str(file.with_suffix(\".onnx\"))\n",
    "    output_names = [\"output0\", \"output1\"] if isinstance(model, SegmentationModel) else [\"output0\"]\n",
    "    if dynamic:\n",
    "        dynamic = {\"images\": {0: \"batch\", 2: \"height\", 3: \"width\"}}  # shape(1,3,640,640)\n",
    "        if isinstance(model, SegmentationModel):\n",
    "            dynamic[\"output0\"] = {0: \"batch\", 1: \"anchors\"}  # shape(1,25200,85)\n",
    "            dynamic[\"output1\"] = {0: \"batch\", 2: \"mask_height\", 3: \"mask_width\"}  # shape(1,32,160,160)\n",
    "        elif isinstance(model, DetectionModel):\n",
    "            dynamic[\"output0\"] = {0: \"batch\", 1: \"anchors\"}  # shape(1,25200,85)\n",
    "    torch.onnx.export(\n",
    "        model.cpu() if dynamic else model,  # --dynamic only compatible with cpu\n",
    "        im.cpu() if dynamic else im,\n",
    "        f,\n",
    "        verbose=False,\n",
    "        opset_version=opset,\n",
    "        do_constant_folding=True,  # WARNING: DNN inference with torch>=1.12 may require do_constant_folding=False\n",
    "        input_names=[\"images\"],\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic or None,\n",
    "    )\n",
    "\n",
    "    # Checks\n",
    "    model_onnx = onnx.load(f)  # load onnx model\n",
    "    onnx.checker.check_model(model_onnx)  # check onnx model\n",
    "\n",
    "    # Metadata\n",
    "    d = {\"stride\": int(max(model.stride)), \"names\": model.names}\n",
    "    for k, v in d.items():\n",
    "        meta = model_onnx.metadata_props.add()\n",
    "        meta.key, meta.value = k, str(v)\n",
    "    onnx.save(model_onnx, f)\n",
    "\n",
    "    # Simplify\n",
    "    if simplify:\n",
    "        try:\n",
    "            cuda = torch.cuda.is_available()\n",
    "            check_requirements((\"onnxruntime-gpu\" if cuda else \"onnxruntime\", \"onnx-simplifier>=0.4.1\"))\n",
    "            import onnxsim\n",
    "            LOGGER.info(f\"{prefix} simplifying with onnx-simplifier {onnxsim.__version__}...\")\n",
    "            model_onnx, check = onnxsim.simplify(model_onnx)\n",
    "            assert check, \"assert check failed\"\n",
    "            onnx.save(model_onnx, f)\n",
    "        except Exception as e:\n",
    "            LOGGER.info(f\"{prefix} simplifier failure: {e}\")\n",
    "    return f, model_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@try_export\n",
    "def export_openvino(file, metadata, half, int8, data, prefix=colorstr(\"OpenVINO:\")):\n",
    "    # YOLOv5 OpenVINO export\n",
    "    check_requirements(\"openvino-dev>=2023.0\")  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n",
    "    import openvino.runtime as ov  # noqa\n",
    "    from openvino.tools import mo  # noqa\n",
    "    LOGGER.info(f\"\\n{prefix} starting export with openvino {ov.__version__}...\")\n",
    "    f = str(file).replace(file.suffix, f\"_{'int8_' if int8 else ''}openvino_model{os.sep}\")\n",
    "    f_onnx = file.with_suffix(\".onnx\")\n",
    "    f_ov = str(Path(f) / file.with_suffix(\".xml\").name)\n",
    "    ov_model = mo.convert_model(f_onnx, model_name=file.stem, framework=\"onnx\", compress_to_fp16=half)  # export\n",
    "    if int8:\n",
    "        check_requirements(\"nncf>=2.5.0\")  # requires at least version 2.5.0 to use the post-training quantization\n",
    "        import nncf\n",
    "        import numpy as np\n",
    "        from utils.dataloaders import create_dataloader\n",
    "        def gen_dataloader(yaml_path, task=\"train\", imgsz=640, workers=4):\n",
    "            data_yaml = check_yaml(yaml_path)\n",
    "            data = check_dataset(data_yaml)\n",
    "            dataloader = create_dataloader(\n",
    "                data[task], imgsz=imgsz, batch_size=1, stride=32, pad=0.5, single_cls=False, rect=False, workers=workers\n",
    "            )[0]\n",
    "            return dataloader\n",
    "\n",
    "        # noqa: F811\n",
    "        def transform_fn(data_item):\n",
    "            \"\"\"\n",
    "            Quantization transform function.\n",
    "            Extracts and preprocess input data from dataloader item for quantization.\n",
    "            Parameters:\n",
    "               data_item: Tuple with data item produced by DataLoader during iteration\n",
    "            Returns:\n",
    "                input_tensor: Input data for quantization\n",
    "            \"\"\"\n",
    "            assert data_item[0].dtype == torch.uint8, \"input image must be uint8 for the quantization preprocessing\"\n",
    "            img = data_item[0].numpy().astype(np.float32)  # uint8 to fp16/32\n",
    "            img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "            return np.expand_dims(img, 0) if img.ndim == 3 else img\n",
    "        ds = gen_dataloader(data)\n",
    "        quantization_dataset = nncf.Dataset(ds, transform_fn)\n",
    "        ov_model = nncf.quantize(ov_model, quantization_dataset, preset=nncf.QuantizationPreset.MIXED)\n",
    "    ov.serialize(ov_model, f_ov)  # save\n",
    "    yaml_save(Path(f) / file.with_suffix(\".yaml\").name, metadata)  # add metadata.yaml\n",
    "    return f, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@try_export\n",
    "def export_paddle(model, im, file, metadata, prefix=colorstr(\"PaddlePaddle:\")):\n",
    "    \"\"\"Exports a YOLOv5 model to PaddlePaddle format using X2Paddle, saving to `save_dir` and adding a metadata.yaml\n",
    "    file.\n",
    "    \"\"\"\n",
    "    check_requirements((\"paddlepaddle\", \"x2paddle\"))\n",
    "    import x2paddle\n",
    "    from x2paddle.convert import pytorch2paddle\n",
    "    LOGGER.info(f\"\\n{prefix} starting export with X2Paddle {x2paddle.__version__}...\")\n",
    "    f = str(file).replace(\".pt\", f\"_paddle_model{os.sep}\")\n",
    "    pytorch2paddle(module=model, save_dir=f, jit_type=\"trace\", input_examples=[im])  # export\n",
    "    yaml_save(Path(f) / file.with_suffix(\".yaml\").name, metadata)  # add metadata.yaml\n",
    "    return f, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@try_export\n",
    "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n",
    "    \"\"\"Exports YOLOv5 model to CoreML format with optional NMS, INT8, and FP16 support; requires coremltools.\"\"\"\n",
    "    check_requirements(\"coremltools\")\n",
    "    import coremltools as ct\n",
    "    LOGGER.info(f\"\\n{prefix} starting export with coremltools {ct.__version__}...\")\n",
    "    f = file.with_suffix(\".mlmodel\")\n",
    "    if nms:\n",
    "        model = iOSModel(model, im)\n",
    "    ts = torch.jit.trace(model, im, strict=False)  # TorchScript model\n",
    "    ct_model = ct.convert(ts, inputs=[ct.ImageType(\"image\", shape=im.shape, scale=1 / 255, bias=[0, 0, 0])])\n",
    "    bits, mode = (8, \"kmeans_lut\") if int8 else (16, \"linear\") if half else (32, None)\n",
    "    if bits < 32:\n",
    "        if MACOS:  # quantization only supported on macOS\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # suppress numpy==1.20 float warning\n",
    "                ct_model = ct.models.neural_network.quantization_utils.quantize_weights(ct_model, bits, mode)\n",
    "        else:\n",
    "            print(f\"{prefix} quantization only supported on macOS, skipping...\")\n",
    "    ct_model.save(f)\n",
    "    return f, ct_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@try_export\n",
    "def export_engine(model, im, file, half, dynamic, simplify, workspace=4, verbose=False, prefix=colorstr(\"TensorRT:\")):\n",
    "    \"\"\"\n",
    "    Exports a YOLOv5 model to TensorRT engine format, requiring GPU and TensorRT>=7.0.0.\n",
    "    https://developer.nvidia.com/tensorrt\n",
    "    \"\"\"\n",
    "    assert im.device.type != \"cpu\", \"export running on CPU but must be on GPU, i.e. `python export.py --device 0`\"\n",
    "    try:\n",
    "        import tensorrt as trt\n",
    "    except Exception:\n",
    "        if platform.system() == \"Linux\":\n",
    "            check_requirements(\"nvidia-tensorrt\", cmds=\"-U --index-url https://pypi.ngc.nvidia.com\")\n",
    "        import tensorrt as trt\n",
    "    if trt.__version__[0] == \"7\":  # TensorRT 7 handling https://github.com/ultralytics/yolov5/issues/6012\n",
    "        grid = model.model[-1].anchor_grid\n",
    "        model.model[-1].anchor_grid = [a[..., :1, :1, :] for a in grid]\n",
    "        export_onnx(model, im, file, 12, dynamic, simplify)  # opset 12\n",
    "        model.model[-1].anchor_grid = grid\n",
    "    else:  # TensorRT >= 8\n",
    "        check_version(trt.__version__, \"8.0.0\", hard=True)  # require tensorrt>=8.0.0\n",
    "        export_onnx(model, im, file, 12, dynamic, simplify)  # opset 12\n",
    "    onnx = file.with_suffix(\".onnx\")\n",
    "    LOGGER.info(f\"\\n{prefix} starting export with TensorRT {trt.__version__}...\")\n",
    "    assert onnx.exists(), f\"failed to export ONNX file: {onnx}\"\n",
    "    f = file.with_suffix(\".engine\")  # TensorRT engine file\n",
    "    logger = trt.Logger(trt.Logger.INFO)\n",
    "    if verbose:\n",
    "        logger.min_severity = trt.Logger.Severity.VERBOSE\n",
    "    builder = trt.Builder(logger)\n",
    "    config = builder.create_builder_config()\n",
    "    config.max_workspace_size = workspace * 1 << 30\n",
    "    # config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace << 30)  # fix TRT 8.4 deprecation notice\n",
    "    flag = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "    network = builder.create_network(flag)\n",
    "    parser = trt.OnnxParser(network, logger)\n",
    "    if not parser.parse_from_file(str(onnx)):\n",
    "        raise RuntimeError(f\"failed to load ONNX file: {onnx}\")\n",
    "    inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
    "    outputs = [network.get_output(i) for i in range(network.num_outputs)]\n",
    "    for inp in inputs:\n",
    "        LOGGER.info(f'{prefix} input \"{inp.name}\" with shape{inp.shape} {inp.dtype}')\n",
    "    for out in outputs:\n",
    "        LOGGER.info(f'{prefix} output \"{out.name}\" with shape{out.shape} {out.dtype}')\n",
    "    if dynamic:\n",
    "        if im.shape[0] <= 1:\n",
    "            LOGGER.warning(f\"{prefix} WARNING âš ï¸ --dynamic model requires maximum --batch-size argument\")\n",
    "        profile = builder.create_optimization_profile()\n",
    "        for inp in inputs:\n",
    "            profile.set_shape(inp.name, (1, *im.shape[1:]), (max(1, im.shape[0] // 2), *im.shape[1:]), im.shape)\n",
    "        config.add_optimization_profile(profile)\n",
    "    LOGGER.info(f\"{prefix} building FP{16 if builder.platform_has_fast_fp16 and half else 32} engine as {f}\")\n",
    "    if builder.platform_has_fast_fp16 and half:\n",
    "        config.set_flag(trt.BuilderFlag.FP16)\n",
    "    with builder.build_engine(network, config) as engine, open(f, \"wb\") as t:\n",
    "        t.write(engine.serialize())\n",
    "    return f, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@try_export\n",
    "def export_saved_model(\n",
    "    model,\n",
    "    im,\n",
    "    file,\n",
    "    dynamic,\n",
    "    tf_nms=False,\n",
    "    agnostic_nms=False,\n",
    "    topk_per_class=100,\n",
    "    topk_all=100,\n",
    "    iou_thres=0.45,\n",
    "    conf_thres=0.25,\n",
    "    keras=False,\n",
    "    prefix=colorstr(\"TensorFlow SavedModel:\"),\n",
    "):\n",
    "    # YOLOv5 TensorFlow SavedModel export\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "    except Exception:\n",
    "        check_requirements(f\"tensorflow{'' if torch.cuda.is_available() else '-macos' if MACOS else '-cpu'}<=2.15.1\")\n",
    "        import tensorflow as tf\n",
    "    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "    from models.tf import TFModel\n",
    "    LOGGER.info(f\"\\n{prefix} starting export with tensorflow {tf.__version__}...\")\n",
    "    if tf.__version__ > \"2.13.1\":\n",
    "        helper_url = \"https://github.com/ultralytics/yolov5/issues/12489\"\n",
    "        LOGGER.info(\n",
    "            f\"WARNING âš ï¸ using Tensorflow {tf.__version__} > 2.13.1 might cause issue when exporting the model to tflite {helper_url}\"\n",
    "        )  # handling issue https://github.com/ultralytics/yolov5/issues/12489\n",
    "    f = str(file).replace(\".pt\", \"_saved_model\")\n",
    "    batch_size, ch, *imgsz = list(im.shape)  # BCHW\n",
    "    tf_model = TFModel(cfg=model.yaml, model=model, nc=model.nc, imgsz=imgsz)\n",
    "    im = tf.zeros((batch_size, *imgsz, ch))  # BHWC order for TensorFlow\n",
    "    _ = tf_model.predict(im, tf_nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)\n",
    "    inputs = tf.keras.Input(shape=(*imgsz, ch), batch_size=None if dynamic else batch_size)\n",
    "    outputs = tf_model.predict(inputs, tf_nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)\n",
    "    keras_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    keras_model.trainable = False\n",
    "    keras_model.summary()\n",
    "    if keras:\n",
    "        keras_model.save(f, save_format=\"tf\")\n",
    "    else:\n",
    "        spec = tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype)\n",
    "        m = tf.function(lambda x: keras_model(x))  # full model\n",
    "        m = m.get_concrete_function(spec)\n",
    "        frozen_func = convert_variables_to_constants_v2(m)\n",
    "        tfm = tf.Module()\n",
    "        tfm.__call__ = tf.function(lambda x: frozen_func(x)[:4] if tf_nms else frozen_func(x), [spec])\n",
    "        tfm.__call__(im)\n",
    "        tf.saved_model.save(\n",
    "            tfm,\n",
    "            f,\n",
    "            options=tf.saved_model.SaveOptions(experimental_custom_gradients=False)\n",
    "            if check_version(tf.__version__, \"2.6\")\n",
    "            else tf.saved_model.SaveOptions(),\n",
    "        )\n",
    "    return f, keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@try_export\n",
    "def export_pb(keras_model, file, prefix=colorstr(\"TensorFlow GraphDef:\")):\n",
    "    \"\"\"Exports YOLOv5 model to TensorFlow GraphDef *.pb format; see https://github.com/leimao/Frozen_Graph_TensorFlow for details.\"\"\"\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "    LOGGER.info(f\"\\n{prefix} starting export with tensorflow {tf.__version__}...\")\n",
    "    f = file.with_suffix(\".pb\")\n",
    "    m = tf.function(lambda x: keras_model(x))  # full model\n",
    "    m = m.get_concrete_function(tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype))\n",
    "    frozen_func = convert_variables_to_constants_v2(m)\n",
    "    frozen_func.graph.as_graph_def()\n",
    "    tf.io.write_graph(graph_or_graph_def=frozen_func.graph, logdir=str(f.parent), name=f.name, as_text=False)\n",
    "    return f, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@try_export\n",
    "def export_tflite(\n",
    "    keras_model, im, file, int8, per_tensor, data, nms, agnostic_nms, prefix=colorstr(\"TensorFlow Lite:\")\n",
    "):\n",
    "    # YOLOv5 TensorFlow Lite export\n",
    "    import tensorflow as tf\n",
    "    LOGGER.info(f\"\\n{prefix} starting export with tensorflow {tf.__version__}...\")\n",
    "    batch_size, ch, *imgsz = list(im.shape)  # BCHW\n",
    "    f = str(file).replace(\".pt\", \"-fp16.tflite\")\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "    converter.target_spec.supported_types = [tf.float16]\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    if int8:\n",
    "        from models.tf import representative_dataset_gen\n",
    "        dataset = LoadImages(check_dataset(check_yaml(data))[\"train\"], img_size=imgsz, auto=False)\n",
    "        converter.representative_dataset = lambda: representative_dataset_gen(dataset, ncalib=100)\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.target_spec.supported_types = []\n",
    "        converter.inference_input_type = tf.uint8  # or tf.int8\n",
    "        converter.inference_output_type = tf.uint8  # or tf.int8\n",
    "        converter.experimental_new_quantizer = True\n",
    "        if per_tensor:\n",
    "            converter._experimental_disable_per_channel = True\n",
    "        f = str(file).replace(\".pt\", \"-int8.tflite\")\n",
    "    if nms or agnostic_nms:\n",
    "        converter.target_spec.supported_ops.append(tf.lite.OpsSet.SELECT_TF_OPS)\n",
    "    tflite_model = converter.convert()\n",
    "    open(f, \"wb\").write(tflite_model)\n",
    "    return f, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@try_export\n",
    "def export_edgetpu(file, prefix=colorstr(\"Edge TPU:\")):\n",
    "    \"\"\"\n",
    "    Exports a YOLOv5 model to Edge TPU compatible TFLite format; requires Linux and Edge TPU compiler.\n",
    "    https://coral.ai/docs/edgetpu/models-intro/\n",
    "    \"\"\"\n",
    "    cmd = \"edgetpu_compiler --version\"\n",
    "    help_url = \"https://coral.ai/docs/edgetpu/compiler/\"\n",
    "    assert platform.system() == \"Linux\", f\"export only supported on Linux. See {help_url}\"\n",
    "    if subprocess.run(f\"{cmd} > /dev/null 2>&1\", shell=True).returncode != 0:\n",
    "        LOGGER.info(f\"\\n{prefix} export requires Edge TPU compiler. Attempting install from {help_url}\")\n",
    "        sudo = subprocess.run(\"sudo --version >/dev/null\", shell=True).returncode == 0  # sudo installed on system\n",
    "        for c in (\n",
    "            \"curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\",\n",
    "            'echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list',\n",
    "            \"sudo apt-get update\",\n",
    "            \"sudo apt-get install edgetpu-compiler\",\n",
    "        ):\n",
    "            subprocess.run(c if sudo else c.replace(\"sudo \", \"\"), shell=True, check=True)\n",
    "    ver = subprocess.run(cmd, shell=True, capture_output=True, check=True).stdout.decode().split()[-1]\n",
    "    LOGGER.info(f\"\\n{prefix} starting export with Edge TPU compiler {ver}...\")\n",
    "    f = str(file).replace(\".pt\", \"-int8_edgetpu.tflite\")  # Edge TPU model\n",
    "    f_tfl = str(file).replace(\".pt\", \"-int8.tflite\")  # TFLite model\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"edgetpu_compiler\",\n",
    "            \"-s\",\n",
    "            \"-d\",\n",
    "            \"-k\",\n",
    "            \"10\",\n",
    "            \"--out_dir\",\n",
    "            str(file.parent),\n",
    "            f_tfl,\n",
    "        ],\n",
    "        check=True,\n",
    "    )\n",
    "    return f, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@try_export\n",
    "def export_tfjs(file, int8, prefix=colorstr(\"TensorFlow.js:\")):\n",
    "    \"\"\"Exports a YOLOv5 model to TensorFlow.js format, optionally with uint8 quantization.\"\"\"\n",
    "    check_requirements(\"tensorflowjs\")\n",
    "    import tensorflowjs as tfjs\n",
    "    LOGGER.info(f\"\\n{prefix} starting export with tensorflowjs {tfjs.__version__}...\")\n",
    "    f = str(file).replace(\".pt\", \"_web_model\")  # js dir\n",
    "    f_pb = file.with_suffix(\".pb\")  # *.pb path\n",
    "    f_json = f\"{f}/model.json\"  # *.json path\n",
    "    args = [\n",
    "        \"tensorflowjs_converter\",\n",
    "        \"--input_format=tf_frozen_model\",\n",
    "        \"--quantize_uint8\" if int8 else \"\",\n",
    "        \"--output_node_names=Identity,Identity_1,Identity_2,Identity_3\",\n",
    "        str(f_pb),\n",
    "        f,\n",
    "    ]\n",
    "    subprocess.run([arg for arg in args if arg], check=True)\n",
    "    json = Path(f_json).read_text()\n",
    "    with open(f_json, \"w\") as j:  # sort JSON Identity_* in ascending order\n",
    "        subst = re.sub(\n",
    "            r'{\"outputs\": {\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n",
    "            r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n",
    "            r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n",
    "            r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}}}',\n",
    "            r'{\"outputs\": {\"Identity\": {\"name\": \"Identity\"}, '\n",
    "            r'\"Identity_1\": {\"name\": \"Identity_1\"}, '\n",
    "            r'\"Identity_2\": {\"name\": \"Identity_2\"}, '\n",
    "            r'\"Identity_3\": {\"name\": \"Identity_3\"}}}',\n",
    "            json,\n",
    "        )\n",
    "        j.write(subst)\n",
    "    return f, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_tflite_metadata(file, metadata, num_outputs):\n",
    "    \"\"\"\n",
    "    Adds TFLite metadata to a model file, supporting multiple outputs, as specified by TensorFlow guidelines.\n",
    "    https://www.tensorflow.org/lite/models/convert/metadata\n",
    "    \"\"\"\n",
    "    with contextlib.suppress(ImportError):\n",
    "        # check_requirements('tflite_support')\n",
    "        from tflite_support import flatbuffers\n",
    "        from tflite_support import metadata as _metadata\n",
    "        from tflite_support import metadata_schema_py_generated as _metadata_fb\n",
    "        tmp_file = Path(\"/tmp/meta.txt\")\n",
    "        with open(tmp_file, \"w\") as meta_f:\n",
    "            meta_f.write(str(metadata))\n",
    "        model_meta = _metadata_fb.ModelMetadataT()\n",
    "        label_file = _metadata_fb.AssociatedFileT()\n",
    "        label_file.name = tmp_file.name\n",
    "        model_meta.associatedFiles = [label_file]\n",
    "        subgraph = _metadata_fb.SubGraphMetadataT()\n",
    "        subgraph.inputTensorMetadata = [_metadata_fb.TensorMetadataT()]\n",
    "        subgraph.outputTensorMetadata = [_metadata_fb.TensorMetadataT()] * num_outputs\n",
    "        model_meta.subgraphMetadata = [subgraph]\n",
    "        b = flatbuffers.Builder(0)\n",
    "        b.Finish(model_meta.Pack(b), _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\n",
    "        metadata_buf = b.Output()\n",
    "        populator = _metadata.MetadataPopulator.with_model_file(file)\n",
    "        populator.load_metadata_buffer(metadata_buf)\n",
    "        populator.load_associated_files([str(tmp_file)])\n",
    "        populator.populate()\n",
    "        tmp_file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pipeline_coreml(model, im, file, names, y, prefix=colorstr(\"CoreML Pipeline:\")):\n",
    "    \"\"\"Converts a PyTorch YOLOv5 model to CoreML format with NMS, handling different input/output shapes and saving the\n",
    "    model.\n",
    "    \"\"\"\n",
    "    import coremltools as ct\n",
    "    from PIL import Image\n",
    "    print(f\"{prefix} starting pipeline with coremltools {ct.__version__}...\")\n",
    "    batch_size, ch, h, w = list(im.shape)  # BCHW\n",
    "    t = time.time()\n",
    "\n",
    "    # YOLOv5 Output shapes\n",
    "    spec = model.get_spec()\n",
    "    out0, out1 = iter(spec.description.output)\n",
    "    if platform.system() == \"Darwin\":\n",
    "        img = Image.new(\"RGB\", (w, h))  # img(192 width, 320 height)\n",
    "        # img = torch.zeros((*opt.img_size, 3)).numpy()  # img size(320,192,3) iDetection\n",
    "        out = model.predict({\"image\": img})\n",
    "        out0_shape, out1_shape = out[out0.name].shape, out[out1.name].shape\n",
    "    else:  # linux and windows can not run model.predict(), get sizes from pytorch output y\n",
    "        s = tuple(y[0].shape)\n",
    "        out0_shape, out1_shape = (s[1], s[2] - 5), (s[1], 4)  # (3780, 80), (3780, 4)\n",
    "\n",
    "    # Checks\n",
    "    nx, ny = spec.description.input[0].type.imageType.width, spec.description.input[0].type.imageType.height\n",
    "    na, nc = out0_shape\n",
    "    # na, nc = out0.type.multiArrayType.shape  # number anchors, classes\n",
    "    assert len(names) == nc, f\"{len(names)} names found for nc={nc}\"  # check\n",
    "\n",
    "    # Define output shapes (missing)\n",
    "    out0.type.multiArrayType.shape[:] = out0_shape  # (3780, 80)\n",
    "    out1.type.multiArrayType.shape[:] = out1_shape  # (3780, 4)\n",
    "    # spec.neuralNetwork.preprocessing[0].featureName = '0'\n",
    "\n",
    "    # Flexible input shapes\n",
    "    # from coremltools.models.neural_network import flexible_shape_utils\n",
    "    # s = [] # shapes\n",
    "    # s.append(flexible_shape_utils.NeuralNetworkImageSize(320, 192))\n",
    "    # s.append(flexible_shape_utils.NeuralNetworkImageSize(640, 384))  # (height, width)\n",
    "    # flexible_shape_utils.add_enumerated_image_sizes(spec, feature_name='image', sizes=s)\n",
    "    # r = flexible_shape_utils.NeuralNetworkImageSizeRange()  # shape ranges\n",
    "    # r.add_height_range((192, 640))\n",
    "    # r.add_width_range((192, 640))\n",
    "    # flexible_shape_utils.update_image_size_range(spec, feature_name='image', size_range=r)\n",
    "\n",
    "    # Print\n",
    "    print(spec.description)\n",
    "\n",
    "    # Model from spec\n",
    "    model = ct.models.MLModel(spec)\n",
    "\n",
    "    # 3. Create NMS protobuf\n",
    "    nms_spec = ct.proto.Model_pb2.Model()\n",
    "    nms_spec.specificationVersion = 5\n",
    "    for i in range(2):\n",
    "        decoder_output = model._spec.description.output[i].SerializeToString()\n",
    "        nms_spec.description.input.add()\n",
    "        nms_spec.description.input[i].ParseFromString(decoder_output)\n",
    "        nms_spec.description.output.add()\n",
    "        nms_spec.description.output[i].ParseFromString(decoder_output)\n",
    "    nms_spec.description.output[0].name = \"confidence\"\n",
    "    nms_spec.description.output[1].name = \"coordinates\"\n",
    "    output_sizes = [nc, 4]\n",
    "    for i in range(2):\n",
    "        ma_type = nms_spec.description.output[i].type.multiArrayType\n",
    "        ma_type.shapeRange.sizeRanges.add()\n",
    "        ma_type.shapeRange.sizeRanges[0].lowerBound = 0\n",
    "        ma_type.shapeRange.sizeRanges[0].upperBound = -1\n",
    "        ma_type.shapeRange.sizeRanges.add()\n",
    "        ma_type.shapeRange.sizeRanges[1].lowerBound = output_sizes[i]\n",
    "        ma_type.shapeRange.sizeRanges[1].upperBound = output_sizes[i]\n",
    "        del ma_type.shape[:]\n",
    "    nms = nms_spec.nonMaximumSuppression\n",
    "    nms.confidenceInputFeatureName = out0.name  # 1x507x80\n",
    "    nms.coordinatesInputFeatureName = out1.name  # 1x507x4\n",
    "    nms.confidenceOutputFeatureName = \"confidence\"\n",
    "    nms.coordinatesOutputFeatureName = \"coordinates\"\n",
    "    nms.iouThresholdInputFeatureName = \"iouThreshold\"\n",
    "    nms.confidenceThresholdInputFeatureName = \"confidenceThreshold\"\n",
    "    nms.iouThreshold = 0.45\n",
    "    nms.confidenceThreshold = 0.25\n",
    "    nms.pickTop.perClass = True\n",
    "    nms.stringClassLabels.vector.extend(names.values())\n",
    "    nms_model = ct.models.MLModel(nms_spec)\n",
    "\n",
    "    # 4. Pipeline models together\n",
    "    pipeline = ct.models.pipeline.Pipeline(\n",
    "        input_features=[\n",
    "            (\"image\", ct.models.datatypes.Array(3, ny, nx)),\n",
    "            (\"iouThreshold\", ct.models.datatypes.Double()),\n",
    "            (\"confidenceThreshold\", ct.models.datatypes.Double()),\n",
    "        ],\n",
    "        output_features=[\"confidence\", \"coordinates\"],\n",
    "    )\n",
    "    pipeline.add_model(model)\n",
    "    pipeline.add_model(nms_model)\n",
    "\n",
    "    # Correct datatypes\n",
    "    pipeline.spec.description.input[0].ParseFromString(model._spec.description.input[0].SerializeToString())\n",
    "    pipeline.spec.description.output[0].ParseFromString(nms_model._spec.description.output[0].SerializeToString())\n",
    "    pipeline.spec.description.output[1].ParseFromString(nms_model._spec.description.output[1].SerializeToString())\n",
    "\n",
    "    # Update metadata\n",
    "    pipeline.spec.specificationVersion = 5\n",
    "    pipeline.spec.description.metadata.versionString = \"https://github.com/ultralytics/yolov5\"\n",
    "    pipeline.spec.description.metadata.shortDescription = \"https://github.com/ultralytics/yolov5\"\n",
    "    pipeline.spec.description.metadata.author = \"glenn.jocher@ultralytics.com\"\n",
    "    pipeline.spec.description.metadata.license = \"https://github.com/ultralytics/yolov5/blob/master/LICENSE\"\n",
    "    pipeline.spec.description.metadata.userDefined.update(\n",
    "        {\n",
    "            \"classes\": \",\".join(names.values()),\n",
    "            \"iou_threshold\": str(nms.iouThreshold),\n",
    "            \"confidence_threshold\": str(nms.confidenceThreshold),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    f = file.with_suffix(\".mlmodel\")  # filename\n",
    "    model = ct.models.MLModel(pipeline.spec)\n",
    "    model.input_description[\"image\"] = \"Input image\"\n",
    "    model.input_description[\"iouThreshold\"] = f\"(optional) IOU Threshold override (default: {nms.iouThreshold})\"\n",
    "    model.input_description[\"confidenceThreshold\"] = (\n",
    "        f\"(optional) Confidence Threshold override (default: {nms.confidenceThreshold})\"\n",
    "    )\n",
    "    model.output_description[\"confidence\"] = 'Boxes Ã— Class confidence (see user-defined metadata \"classes\")'\n",
    "    model.output_description[\"coordinates\"] = \"Boxes Ã— [x, y, width, height] (relative to image size)\"\n",
    "    model.save(f)  # pipelined\n",
    "    print(f\"{prefix} pipeline success ({time.time() - t:.2f}s), saved as {f} ({file_size(f):.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@smart_inference_mode()\n",
    "def run(\n",
    "    data=ROOT / \"data/custom_dataset.yaml\",  # 'dataset.yaml path'\n",
    "    weights=ROOT / \"runs/train/exp_fotosarneses2/best.pt\",  # weights path\n",
    "    imgsz=(640, 640),  # image (height, width)\n",
    "    batch_size=1,  # batch size\n",
    "    device=\"cpu\",  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "    include=(\"onnx\", \"tflite\"),  # include formats\n",
    "    half=False,  # FP16 half-precision export\n",
    "    inplace=False,  # set YOLOv5 Detect() inplace=True\n",
    "    keras=False,  # use Keras\n",
    "    optimize=False,  # TorchScript: optimize for mobile\n",
    "    int8=False,  # CoreML/TF INT8 quantization\n",
    "    per_tensor=False,  # TF per tensor quantization\n",
    "    dynamic=False,  # ONNX/TF/TensorRT: dynamic axes\n",
    "    simplify=False,  # ONNX: simplify model\n",
    "    opset=12,  # ONNX: opset version\n",
    "    verbose=False,  # TensorRT: verbose log\n",
    "    workspace=4,  # TensorRT: workspace size (GB)\n",
    "    nms=False,  # TF: add NMS to model\n",
    "    agnostic_nms=False,  # TF: add agnostic NMS to model\n",
    "    topk_per_class=100,  # TF.js NMS: topk per class to keep\n",
    "    topk_all=100,  # TF.js NMS: topk for all classes to keep\n",
    "    iou_thres=0.45,  # TF.js NMS: IoU threshold\n",
    "    conf_thres=0.25,  # TF.js NMS: confidence threshold\n",
    "):\n",
    "    t = time.time()\n",
    "    include = [x.lower() for x in include]  # to lowercase\n",
    "    fmts = tuple(export_formats()[\"Argument\"][1:])  # --include arguments\n",
    "    flags = [x in include for x in fmts]\n",
    "    assert sum(flags) == len(include), f\"ERROR: Invalid --include {include}, valid --include arguments are {fmts}\"\n",
    "    jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle = flags  # export booleans\n",
    "    file = Path(url2file(weights) if str(weights).startswith((\"http:/\", \"https:/\")) else weights)  # PyTorch weights\n",
    "\n",
    "    # Load PyTorch model\n",
    "    device = select_device(device)\n",
    "    if half:\n",
    "        assert device.type != \"cpu\" or coreml, \"--half only compatible with GPU export, i.e. use --device 0\"\n",
    "        assert not dynamic, \"--half not compatible with --dynamic, i.e. use either --half or --dynamic but not both\"\n",
    "    model = attempt_load(weights, device=device, inplace=True, fuse=True)  # load FP32 model\n",
    "\n",
    "    # Checks\n",
    "    imgsz *= 2 if len(imgsz) == 1 else 1  # expand\n",
    "    if optimize:\n",
    "        assert device.type == \"cpu\", \"--optimize not compatible with cuda devices, i.e. use --device cpu\"\n",
    "\n",
    "    # Input\n",
    "    gs = int(max(model.stride))  # grid size (max stride)\n",
    "    imgsz = [check_img_size(x, gs) for x in imgsz]  # verify img_size are gs-multiples\n",
    "    im = torch.zeros(batch_size, 3, *imgsz).to(device)  # image size(1,3,320,192) BCHW iDetection\n",
    "\n",
    "    # Update model\n",
    "    model.eval()\n",
    "    for k, m in model.named_modules():\n",
    "        if isinstance(m, Detect):\n",
    "            m.inplace = inplace\n",
    "            m.dynamic = dynamic\n",
    "            m.export = True\n",
    "    for _ in range(2):\n",
    "        y = model(im)  # dry runs\n",
    "    if half and not coreml:\n",
    "        im, model = im.half(), model.half()  # to FP16\n",
    "    shape = tuple((y[0] if isinstance(y, tuple) else y).shape)  # model output shape\n",
    "    metadata = {\"stride\": int(max(model.stride)), \"names\": model.names}  # model metadata\n",
    "    LOGGER.info(f\"\\n{colorstr('PyTorch:')} starting from {file} with output shape {shape} ({file_size(file):.1f} MB)\")\n",
    "\n",
    "    # Exports\n",
    "    f = [\"\"] * len(fmts)  # exported filenames\n",
    "    warnings.filterwarnings(action=\"ignore\", category=torch.jit.TracerWarning)  # suppress TracerWarning\n",
    "    if jit:  # TorchScript\n",
    "        f[0], _ = export_torchscript(model, im, file, optimize)\n",
    "    if engine:  # TensorRT required before ONNX\n",
    "        f[1], _ = export_engine(model, im, file, half, dynamic, simplify, workspace, verbose)\n",
    "    if onnx or xml:  # OpenVINO requires ONNX\n",
    "        f[2], _ = export_onnx(model, im, file, opset, dynamic, simplify)\n",
    "    if xml:  # OpenVINO\n",
    "        f[3], _ = export_openvino(file, metadata, half, int8, data)\n",
    "    if coreml:  # CoreML\n",
    "        f[4], ct_model = export_coreml(model, im, file, int8, half, nms)\n",
    "        if nms:\n",
    "            pipeline_coreml(ct_model, im, file, model.names, y)\n",
    "    if any((saved_model, pb, tflite, edgetpu, tfjs)):  # TensorFlow formats\n",
    "        assert not tflite or not tfjs, \"TFLite and TF.js models must be exported separately, please pass only one type.\"\n",
    "        assert not isinstance(model, ClassificationModel), \"ClassificationModel export to TF formats not yet supported.\"\n",
    "        f[5], s_model = export_saved_model(\n",
    "            model.cpu(),\n",
    "            im,\n",
    "            file,\n",
    "            dynamic,\n",
    "            tf_nms=nms or agnostic_nms or tfjs,\n",
    "            agnostic_nms=agnostic_nms or tfjs,\n",
    "            topk_per_class=topk_per_class,\n",
    "            topk_all=topk_all,\n",
    "            iou_thres=iou_thres,\n",
    "            conf_thres=conf_thres,\n",
    "            keras=keras,\n",
    "        )\n",
    "        if pb or tfjs:  # pb prerequisite to tfjs\n",
    "            f[6], _ = export_pb(s_model, file)\n",
    "        if tflite or edgetpu:\n",
    "            f[7], _ = export_tflite(\n",
    "                s_model, im, file, int8 or edgetpu, per_tensor, data=data, nms=nms, agnostic_nms=agnostic_nms\n",
    "            )\n",
    "            if edgetpu:\n",
    "                f[8], _ = export_edgetpu(file)\n",
    "            add_tflite_metadata(f[8] or f[7], metadata, num_outputs=len(s_model.outputs))\n",
    "        if tfjs:\n",
    "            f[9], _ = export_tfjs(file, int8)\n",
    "    if paddle:  # PaddlePaddle\n",
    "        f[10], _ = export_paddle(model, im, file, metadata)\n",
    "\n",
    "    # Finish\n",
    "    f = [str(x) for x in f if x]  # filter out '' and None\n",
    "    if any(f):\n",
    "        cls, det, seg = (isinstance(model, x) for x in (ClassificationModel, DetectionModel, SegmentationModel))  # type\n",
    "        det &= not seg  # segmentation models inherit from SegmentationModel(DetectionModel)\n",
    "        dir = Path(\"segment\" if seg else \"classify\" if cls else \"\")\n",
    "        h = \"--half\" if half else \"\"  # --half FP16 inference arg\n",
    "        s = (\n",
    "            \"# WARNING âš ï¸ ClassificationModel not yet supported for PyTorch Hub AutoShape inference\"\n",
    "            if cls\n",
    "            else \"# WARNING âš ï¸ SegmentationModel not yet supported for PyTorch Hub AutoShape inference\"\n",
    "            if seg\n",
    "            else \"\"\n",
    "        )\n",
    "        LOGGER.info(\n",
    "            f'\\nExport complete ({time.time() - t:.1f}s)'\n",
    "            f\"\\nResults saved to {colorstr('bold', file.parent.resolve())}\"\n",
    "            f\"\\nDetect:          python {dir / ('detect.py' if det else 'predict.py')} --weights {f[-1]} {h}\"\n",
    "            f\"\\nValidate:        python {dir / 'val.py'} --weights {f[-1]} {h}\"\n",
    "            f\"\\nPyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', '{f[-1]}')  {s}\"\n",
    "            f'\\nVisualize:       https://netron.app'\n",
    "        )\n",
    "    return f  # return list of exported files/dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def parse_opt(known=False):\n",
    "#     \"\"\"Parses command-line arguments for YOLOv5 model export configurations, returning the parsed options.\"\"\"\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--data\", type=str, default=ROOT / \"data/coco128.yaml\", help=\"dataset.yaml path\")\n",
    "#     parser.add_argument(\"--weights\", nargs=\"+\", type=str, default=ROOT / \"yolov5s.pt\", help=\"model.pt path(s)\")\n",
    "#     parser.add_argument(\"--imgsz\", \"--img\", \"--img-size\", nargs=\"+\", type=int, default=[640, 640], help=\"image (h, w)\")\n",
    "#     parser.add_argument(\"--batch-size\", type=int, default=1, help=\"batch size\")\n",
    "#     parser.add_argument(\"--device\", default=\"cpu\", help=\"cuda device, i.e. 0 or 0,1,2,3 or cpu\")\n",
    "#     parser.add_argument(\"--half\", action=\"store_true\", help=\"FP16 half-precision export\")\n",
    "#     parser.add_argument(\"--inplace\", action=\"store_true\", help=\"set YOLOv5 Detect() inplace=True\")\n",
    "#     parser.add_argument(\"--keras\", action=\"store_true\", help=\"TF: use Keras\")\n",
    "#     parser.add_argument(\"--optimize\", action=\"store_true\", help=\"TorchScript: optimize for mobile\")\n",
    "#     parser.add_argument(\"--int8\", action=\"store_true\", help=\"CoreML/TF/OpenVINO INT8 quantization\")\n",
    "#     parser.add_argument(\"--per-tensor\", action=\"store_true\", help=\"TF per-tensor quantization\")\n",
    "#     parser.add_argument(\"--dynamic\", action=\"store_true\", help=\"ONNX/TF/TensorRT: dynamic axes\")\n",
    "#     parser.add_argument(\"--simplify\", action=\"store_true\", help=\"ONNX: simplify model\")\n",
    "#     parser.add_argument(\"--opset\", type=int, default=17, help=\"ONNX: opset version\")\n",
    "#     parser.add_argument(\"--verbose\", action=\"store_true\", help=\"TensorRT: verbose log\")\n",
    "#     parser.add_argument(\"--workspace\", type=int, default=4, help=\"TensorRT: workspace size (GB)\")\n",
    "#     parser.add_argument(\"--nms\", action=\"store_true\", help=\"TF: add NMS to model\")\n",
    "#     parser.add_argument(\"--agnostic-nms\", action=\"store_true\", help=\"TF: add agnostic NMS to model\")\n",
    "#     parser.add_argument(\"--topk-per-class\", type=int, default=100, help=\"TF.js NMS: topk per class to keep\")\n",
    "#     parser.add_argument(\"--topk-all\", type=int, default=100, help=\"TF.js NMS: topk for all classes to keep\")\n",
    "#     parser.add_argument(\"--iou-thres\", type=float, default=0.45, help=\"TF.js NMS: IoU threshold\")\n",
    "#     parser.add_argument(\"--conf-thres\", type=float, default=0.25, help=\"TF.js NMS: confidence threshold\")\n",
    "#     parser.add_argument(\n",
    "#         \"--include\",\n",
    "#         nargs=\"+\",\n",
    "#         default=[\"torchscript\"],\n",
    "#         help=\"torchscript, onnx, openvino, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle\",\n",
    "#     )\n",
    "#     opt = parser.parse_known_args()[0] if known else parser.parse_args()\n",
    "#     print_args(vars(opt))\n",
    "#     return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Executes the YOLOv5 model inference or export with specified weights and options.\"\"\"\n",
    "    #for opt.weights in opt.weights if isinstance(opt.weights, list) else [opt.weights]:\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v7.0-294-gdb125a20 Python-3.10.13 torch-2.1.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 1765930 parameters, 0 gradients, 4.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from runs/train/exp_fotosarneses2/best.pt with output shape (1, 25200, 10) (3.7 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.13.1...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.3s, saved as runs/train/exp_fotosarneses2/best.onnx (7.2 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['tensorflow-cpu<=2.15.1'] not found, attempting AutoUpdate...\n",
      "Collecting tensorflow-cpu<=2.15.1\n",
      "  Downloading tensorflow_cpu-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-cpu<=2.15.1)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-cpu<=2.15.1)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-cpu<=2.15.1)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-cpu<=2.15.1)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorflow-cpu<=2.15.1) (0.2.0)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-cpu<=2.15.1)\n",
      "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-cpu<=2.15.1)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow-cpu<=2.15.1)\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorflow-cpu<=2.15.1) (1.26.4)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-cpu<=2.15.1)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorflow-cpu<=2.15.1) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorflow-cpu<=2.15.1) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorflow-cpu<=2.15.1) (69.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorflow-cpu<=2.15.1) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorflow-cpu<=2.15.1) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorflow-cpu<=2.15.1) (4.9.0)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow-cpu<=2.15.1)\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-cpu<=2.15.1)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-cpu<=2.15.1)\n",
      "  Downloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow-cpu<=2.15.1)\n",
      "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow-cpu<=2.15.1)\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow-cpu<=2.15.1)\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-cpu<=2.15.1) (0.42.0)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow-cpu<=2.15.1)\n",
      "  Downloading google_auth-2.29.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow-cpu<=2.15.1)\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.16,>=2.15->tensorflow-cpu<=2.15.1)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-cpu<=2.15.1) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow-cpu<=2.15.1)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow-cpu<=2.15.1) (3.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->tensorflow-cpu<=2.15.1) (3.1.1)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-cpu<=2.15.1)\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-cpu<=2.15.1)\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-cpu<=2.15.1) (4.7.2)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-cpu<=2.15.1)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-cpu<=2.15.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-cpu<=2.15.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-cpu<=2.15.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-cpu<=2.15.1) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-cpu<=2.15.1) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-cpu<=2.15.1) (0.5.1)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-cpu<=2.15.1)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading tensorflow_cpu-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.2/207.2 MB\u001b[0m \u001b[31m250.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m364.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Downloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m344.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m297.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m397.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m266.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m376.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m273.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m312.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m410.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m308.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m332.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m189.2/189.2 kB\u001b[0m \u001b[31m384.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m371.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m390.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m384.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wrapt, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1-modules, opt-einsum, oauthlib, ml-dtypes, markdown, keras, h5py, grpcio, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-cpu\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 flatbuffers-24.3.25 gast-0.5.4 google-auth-2.29.0 google-auth-oauthlib-1.2.0 grpcio-1.62.1 h5py-3.11.0 keras-2.15.0 libclang-18.1.1 markdown-3.6 ml-dtypes-0.3.2 oauthlib-3.2.2 opt-einsum-3.3.0 pyasn1-modules-0.4.0 requests-oauthlib-2.0.0 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-cpu-2.15.1 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.36.0 wrapt-1.14.1\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 17.7s, installed 1 package: ['tensorflow-cpu<=2.15.1']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m âš ï¸ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.15.1...\n",
      "WARNING âš ï¸ using Tensorflow 2.15.1 > 2.13.1 might cause issue when exporting the model to tflite https://github.com/ultralytics/yolov5/issues/12489\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]              \n",
      "  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]                \n",
      "  2                -1  1      4800  models.common.C3                        [32, 32, 1]                   \n",
      "  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  4                -1  1     29184  models.common.C3                        [64, 64, 2]                   \n",
      "  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  6                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  8                -1  1    296448  models.common.C3                        [256, 256, 1]                 \n",
      "  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]                 \n",
      " 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]           \n",
      " 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]          \n",
      " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 24      [17, 20, 23]  1     13530  models.yolo.Detect                      [5, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256], [640, 640]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(1, 640, 640, 3)]           0         []                            \n",
      "                                                                                                  \n",
      " tf_conv (TFConv)            (1, 320, 320, 16)            1744      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf_conv_1 (TFConv)          (1, 160, 160, 32)            4640      ['tf_conv[0][0]']             \n",
      "                                                                                                  \n",
      " tfc3 (TFC3)                 (1, 160, 160, 32)            4704      ['tf_conv_1[0][0]']           \n",
      "                                                                                                  \n",
      " tf_conv_7 (TFConv)          (1, 80, 80, 64)              18496     ['tfc3[0][0]']                \n",
      "                                                                                                  \n",
      " tfc3_1 (TFC3)               (1, 80, 80, 64)              28928     ['tf_conv_7[0][0]']           \n",
      "                                                                                                  \n",
      " tf_conv_15 (TFConv)         (1, 40, 40, 128)             73856     ['tfc3_1[0][0]']              \n",
      "                                                                                                  \n",
      " tfc3_2 (TFC3)               (1, 40, 40, 128)             156288    ['tf_conv_15[0][0]']          \n",
      "                                                                                                  \n",
      " tf_conv_25 (TFConv)         (1, 20, 20, 256)             295168    ['tfc3_2[0][0]']              \n",
      "                                                                                                  \n",
      " tfc3_3 (TFC3)               (1, 20, 20, 256)             295680    ['tf_conv_25[0][0]']          \n",
      "                                                                                                  \n",
      " tfsppf (TFSPPF)             (1, 20, 20, 256)             164224    ['tfc3_3[0][0]']              \n",
      "                                                                                                  \n",
      " tf_conv_33 (TFConv)         (1, 20, 20, 128)             32896     ['tfsppf[0][0]']              \n",
      "                                                                                                  \n",
      " tf_upsample (TFUpsample)    (1, 40, 40, 128)             0         ['tf_conv_33[0][0]']          \n",
      "                                                                                                  \n",
      " tf_concat (TFConcat)        (1, 40, 40, 256)             0         ['tf_upsample[0][0]',         \n",
      "                                                                     'tfc3_2[0][0]']              \n",
      "                                                                                                  \n",
      " tfc3_4 (TFC3)               (1, 40, 40, 128)             90496     ['tf_concat[0][0]']           \n",
      "                                                                                                  \n",
      " tf_conv_39 (TFConv)         (1, 40, 40, 64)              8256      ['tfc3_4[0][0]']              \n",
      "                                                                                                  \n",
      " tf_upsample_1 (TFUpsample)  (1, 80, 80, 64)              0         ['tf_conv_39[0][0]']          \n",
      "                                                                                                  \n",
      " tf_concat_1 (TFConcat)      (1, 80, 80, 128)             0         ['tf_upsample_1[0][0]',       \n",
      "                                                                     'tfc3_1[0][0]']              \n",
      "                                                                                                  \n",
      " tfc3_5 (TFC3)               (1, 80, 80, 64)              22720     ['tf_concat_1[0][0]']         \n",
      "                                                                                                  \n",
      " tf_conv_45 (TFConv)         (1, 40, 40, 64)              36928     ['tfc3_5[0][0]']              \n",
      "                                                                                                  \n",
      " tf_concat_2 (TFConcat)      (1, 40, 40, 128)             0         ['tf_conv_45[0][0]',          \n",
      "                                                                     'tf_conv_39[0][0]']          \n",
      "                                                                                                  \n",
      " tfc3_6 (TFC3)               (1, 40, 40, 128)             74112     ['tf_concat_2[0][0]']         \n",
      "                                                                                                  \n",
      " tf_conv_51 (TFConv)         (1, 20, 20, 128)             147584    ['tfc3_6[0][0]']              \n",
      "                                                                                                  \n",
      " tf_concat_3 (TFConcat)      (1, 20, 20, 256)             0         ['tf_conv_51[0][0]',          \n",
      "                                                                     'tf_conv_33[0][0]']          \n",
      "                                                                                                  \n",
      " tfc3_7 (TFC3)               (1, 20, 20, 256)             295680    ['tf_concat_3[0][0]']         \n",
      "                                                                                                  \n",
      " tf_detect (TFDetect)        ((1, 25200, 10),             13530     ['tfc3_5[0][0]',              \n",
      "                             )                                       'tfc3_6[0][0]',              \n",
      "                                                                     'tfc3_7[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1765930 (6.74 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 1765930 (6.74 MB)\n",
      "__________________________________________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: runs/train/exp_fotosarneses2/best_saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success âœ… 25.3s, saved as runs/train/exp_fotosarneses2/best_saved_model (7.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m starting export with tensorflow 2.15.1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpb_kh54ri/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpb_kh54ri/assets\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 146, Total Ops 535, % non-converted = 27.29 %\n",
      " * 146 ARITH ops\n",
      "\n",
      "- arith.constant:  146 occurrences  (f16: 131, i32: 15)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 10)\n",
      "  (f32: 17)\n",
      "  (f32: 60)\n",
      "  (f32: 131)\n",
      "  (f32: 66)\n",
      "  (f32: 3)\n",
      "  (f32: 75)\n",
      "  (f32: 7)\n",
      "  (f32: 6)\n",
      "  (f32: 2)\n",
      "  (f32: 9)\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m export success âœ… 29.2s, saved as runs/train/exp_fotosarneses2/best-fp16.tflite (3.5 MB)\n",
      "\n",
      "Export complete (59.8s)\n",
      "Results saved to \u001b[1m/home/ec2-user/SageMaker/elecnor/notebooks/ImagenesEPI/yolov5/runs/train/exp_fotosarneses2\u001b[0m\n",
      "Detect:          python detect.py --weights runs/train/exp_fotosarneses2/best-fp16.tflite \n",
      "Validate:        python val.py --weights runs/train/exp_fotosarneses2/best-fp16.tflite \n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train/exp_fotosarneses2/best-fp16.tflite')  \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
